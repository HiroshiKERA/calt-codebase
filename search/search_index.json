{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CALT codebase","text":"<p>The CALT codebase provides a template for generating arithmetic and symbolic computation instances and training Transformer models using the CALT (Computer ALgebra with Transformer) library.</p> <p>While CALT can be installed via <code>pip</code>, the following is the simplest setup your experiment with all dependencies:</p> <pre><code>git clone https://github.com/HiroshiKERA/calt-codebase.git\ncd calt-codebase\nconda env create -f environment.yml \n</code></pre> <p>The documentation of the CALT codebase provides a quickstart guide and tips for organizing your own projects. For detailed usage of the CALT library, please refer to the CALT documentation.</p>"},{"location":"quickstart/","title":"Quick Start","text":"<p>This tutorial provides a minimal introduction on how to build your own project to experiment with the CALT library.</p>"},{"location":"quickstart/#setup","title":"Setup","text":"<p>CALT is based on Python and several popular frameworks including SageMath, PyTorch, and HuggingFace. We provide a conda environment that offers a simple setup.</p> <p>First, install conda enviroment from Anaconda or Miniconda. Then, clone this repository and create an conda enviroment with our enviroment.yml. </p> <pre><code>git clone https://github.com/HiroshiKERA/calt-codebase.git\ncd calt-codebase\nconda env create -f environment.yml  # see environment.yml to define your environment name (default: calt-env)\n</code></pre> <p>After creating the environment, activate it:</p> <pre><code>conda activate calt-env  # replace `calt-env` with the one you named\n</code></pre> <p>The environment is now all set.</p>"},{"location":"quickstart/#first-run","title":"First Run","text":"<p>The codebase is ready to generate a simple dataset and train a Transformer. You can first try this to see if everything works properly.</p>"},{"location":"quickstart/#1-dataset-construction","title":"1. Dataset Construction","text":"<pre><code>python scripts/sagemath/polynomial_problem_generation.py\n</code></pre> <p>By default, this generates train and test sets of the partial polynomial sum task. The generated dataset can be found in <code>dataset/partial_sum</code>.</p>"},{"location":"quickstart/#2-training-transformer","title":"2. Training Transformer","text":"<pre><code>python scripts/sagemath/train.py --config config/train_example.yaml\n</code></pre> <p>This trains a Transformer model with the setup described in <code>config/train_example.yaml</code>. This file specifies training parameters, Transformer architecture, paths to load data, and the directory to save the results and logs. The training process can also be viewed in the WandB platform (the link will be printed once training starts).</p> <p>Note: Initial runs may require your WandB API key. Visit WandB, create your account, and copy &amp; paste your API key to the terminal as required.</p>"},{"location":"quickstart/#your-own-project","title":"Your Own Project","text":"<p>Now you can create custom script files for your own project. In <code>scripts/sagemath/</code>, you can find three script files for dataset construction as examples: one for numerical tasks, another for polynomial tasks, and the last for other tasks.</p> <p>Let's take <code>polynomial_problem_generation.py</code> as an example. You can find two classes: <code>PartialSumProblemGenerator</code> and <code>PolyStatisticsCalculator</code>. The former is the main part of instance generation, and the latter computes statistics of generated instances.</p> <p>Here, the task input is a list of polynomials $F = [f_1, ..., f_s]$, and the expected output is the list of cumulative sums $G = [g_1, ..., g_s]$, where $g_k = f_1 + \\cdots + f_k$.</p> <pre><code>class PartialSumProblemGenerator:\n    def __init__(self, sampler: PolynomialSampler, ...):\n        self.sampler = sampler\n        ...\n\n    def __call__(self, seed: int):\n        # Set random seed for SageMath's random state\n        randstate.set_random_seed(seed)\n\n        # Choose number of polynomials for this sample\n        num_polys = randint(self.min_polynomials, self.max_polynomials)\n\n        # Generate problem polynomials using sampler\n        F = self.sampler.sample(num_samples=num_polys)\n\n        # Generate partial sums for solution\n        G = [sum(F[:i + 1]) for i in range(len(F))]\n\n        return F, G\n</code></pre> <p>This includes three essential components: - <code>sampler</code> for random sampling of polynomials, which is necessary in most cases - <code>seed</code> for reproducibility. Make sure you have this argument in your custom problem generators - <code>__call__</code> for generating an instance. The expected output is a pair of input and output</p> <p>Define your own class by renaming the class and redefining <code>__call__</code>. If needed, also define your own PolyStatisticsCalculator for dataset analysis. Finally, rewrite <code>main()</code> accordingly. </p> <p>If your instance generation requires a sophisticated process, you may add subroutines in the class, and further add some utility files in <code>src/</code>. Below is an example. </p> <pre><code>class GroebnerProblemGenerator:\n    def __init__(self, sampler: PolynomialSampler, max_polynomials: int, min_polynomials: int):\n        self.sampler = sampler\n        self.min_polynomials = min_polynomials\n        self.max_polynomials = max_polynomials\n\n    def __call__(self, seed: int):\n        # Set random seed for SageMath's random state\n        randstate.set_random_seed(seed)\n\n        # Choose number of polynomials for this sample\n        num_polys = randint(self.min_polynomials, self.max_polynomials)\n\n        # Generate problem polynomials using sampler\n        G = self.sampling_groebner_basis()\n        F = self.ideal_invariant_transform(G)\n\n        return F, G\n\n    def sampling_groebner_basis(self):\n        '''Randomly sample a Gr\u00f6bner basis G'''\n        pass\n\n    def ideal_invariant_transform(self, G):\n        '''Generate non-Gr\u00f6bner basis F such that &lt;F&gt; = &lt;G&gt;'''\n        pass\n</code></pre> <p>To train the Transformer model on the custom dataset, rewrite the path name and other dataset setup in <code>config/train_example.yaml</code>:</p> <pre><code>train_dataset_path: data/GB_problem/GF7_n=2/train_raw.txt\ntest_dataset_path: data/GB_problem/GF7_n=2/test_raw.txt\nnum_variables: 2\nmax_degree: 14\nmax_coeff: 10\nfield: GF7\n</code></pre> <p>Note: Make sure that <code>max_coeff</code> and <code>max_degree</code> are large enough to cover all coefficients and degrees appearing in instances. Otherwise, you may encounter tokenization errors with the unknown token <code>[UNK]</code>.</p>"}]}